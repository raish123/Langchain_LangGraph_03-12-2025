{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086386f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: D:\\Langchain_LangGraph_03-12-2025\\MyProject\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook ka folder\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "\n",
    "# Project root = parent folder\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "# Add project root to import path\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41bb197d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-07 22:46:39,375]-config_variable.py-INFO -Loading the environment Variable\n",
      "[2025-12-07 22:46:39,380]-config_variable.py-INFO -Environment Variable successfully Loaded\n"
     ]
    }
   ],
   "source": [
    "#now importing all the Module which is used to build the AI Model.\n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from exception import CustomException\n",
    "from logger_config import logger\n",
    "import os,sys\n",
    "\n",
    "#using openai chat model and embedding models\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "\n",
    "#using groq chat model \n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "#using open source chat model from hugging Face\n",
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEmbeddings,HuggingFaceEndpoint\n",
    "\n",
    "from config import *\n",
    "\n",
    "from langchain_core.runnables import RunnableBranch,RunnableLambda,RunnableParallel,RunnableSequence,RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b0f5c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Langchain_LangGraph_03-12-2025\\\\MyProject\\\\notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7e93f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Langgraph related Modules\n",
    "import langgraph\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from dataclasses import dataclass\n",
    "from typing import TypedDict\n",
    "from typing import Literal,List,Annotated\n",
    "from langchain_core.messages import AnyMessage,AIMessage,HumanMessage,ToolMessage\n",
    "\n",
    "from pydantic import BaseModel,Field #using this class we can perform validation to schema\n",
    "\n",
    "from langgraph.prebuilt import tool_node,tools_condition #in this class we put all tools together\n",
    "#tools_condition wrt to tool msg it will route the flow data to ttol node to perform execution\n",
    "\n",
    "from langchain_core.tools import tool,Tool,StructuredTool\n",
    "\n",
    "from langgraph.graph.message import BaseMessage #this is special class which hold every mesaage init.\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser,PydanticOutputParser\n",
    "\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver #this will save the converstion history in ram memory\n",
    "from langchain_core.runnables import RunnableConfig   #this will help to induce the thread_id or user identification in converstion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5236d8a",
   "metadata": {},
   "source": [
    "## step:1) defining the models components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8971183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001E82B5B9EA0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001E82BC1DFF0>, root_client=<openai.OpenAI object at 0x000001E82B5BA320>, root_async_client=<openai.AsyncOpenAI object at 0x000001E82B5B9FF0>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.2 #we call as creative parameter\n",
    ")\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63626169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001E82BD17010>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001E82BD51AB0>, model_name='llama-3.1-8b-instant', temperature=0.2, model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.2 #we call as creative parameter\n",
    ")\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cd8c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatHuggingFace(llm=HuggingFaceEndpoint(repo_id='meta-llama/Llama-3.1-8B-Instruct', repetition_penalty=1.03, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='meta-llama/Llama-3.1-8B-Instruct', client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, async_client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, task='text-generation'), model_id='meta-llama/Llama-3.1-8B-Instruct', model_kwargs={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(  \n",
    "repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",  \n",
    "task=\"text-generation\",  \n",
    "max_new_tokens=512,  \n",
    "do_sample=False,  \n",
    "repetition_penalty=1.03,  \n",
    ")  \n",
    "\n",
    "model3 = ChatHuggingFace(llm=llm, verbose=True)\n",
    "model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edceede",
   "metadata": {},
   "source": [
    "## building different type of complex workflows using LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0e6ed",
   "metadata": {},
   "source": [
    "### 1) building Persistence Memory workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd338a3b",
   "metadata": {},
   "source": [
    "#### defining state or memory schema for this workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdf7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2dfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71809232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3bfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da549777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723bffe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a26324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67eebd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f706b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ca2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7c161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c12cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f68348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831569c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82f51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f708840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279eaf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86094de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88019f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58f2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e120389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca652290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2f74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fd4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9498f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3657c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9ab4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505b4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a03cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45aca11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae30be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b07c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33d1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb856ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
