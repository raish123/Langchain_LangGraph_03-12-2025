{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086386f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: D:\\Langchain_LangGraph_03-12-2025\\MyProject\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook ka folder\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "\n",
    "# Project root = parent folder\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "# Add project root to import path\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "\n",
    "import warnings as w\n",
    "w.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b98bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nProduction-Grade RAG System with:\\n1. Hybrid Search (Semantic + Keyword)\\n2. Reranking (Cohere)\\n3. LangGraph Integration\\n4. Evaluation Metrics\\n5. Complete Pipeline\\n\\nRun in Jupyter Notebook\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Production-Grade RAG System with:\n",
    "1. Hybrid Search (Semantic + Keyword)\n",
    "2. Reranking (Cohere)\n",
    "3. LangGraph Integration\n",
    "4. Evaluation Metrics\n",
    "5. Complete Pipeline\n",
    "\n",
    "Run in Jupyter Notebook\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66bba19",
   "metadata": {},
   "source": [
    "#### LangChain related Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41bb197d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:52:38,282]-config_variable.py-INFO -Loading the environment Variable\n",
      "[2025-12-13 22:52:38,284]-config_variable.py-INFO -Environment Variable successfully Loaded\n"
     ]
    }
   ],
   "source": [
    "#now importing all the Module which is used to build the AI Model.\n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from exception import CustomException\n",
    "from logger_config import logger\n",
    "import os,sys\n",
    "\n",
    "#using openai chat model and embedding models\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "\n",
    "#using groq chat model \n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "#using open source chat model from hugging Face\n",
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEmbeddings,HuggingFaceEndpoint\n",
    "\n",
    "from config import *\n",
    "\n",
    "from langchain_core.runnables import RunnableBranch,RunnableLambda,RunnableParallel,RunnableSequence,RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0f5c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Langchain_LangGraph_03-12-2025\\\\MyProject\\\\notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62098ec0",
   "metadata": {},
   "source": [
    "#### LanGraph related Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7e93f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Langgraph related Modules\n",
    "import langgraph\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from dataclasses import dataclass\n",
    "from typing import TypedDict\n",
    "from typing import Literal,List,Annotated\n",
    "from langchain_core.messages import AnyMessage,AIMessage,HumanMessage,ToolMessage\n",
    "\n",
    "from pydantic import BaseModel #using this class we can perform validation to schema\n",
    "\n",
    "from langgraph.prebuilt import tool_node,tools_condition #in this class we put all tools together\n",
    "#tools_condition wrt to tool msg it will route the flow data to ttol node to perform execution\n",
    "\n",
    "from langchain_core.tools import tool,Tool,StructuredTool\n",
    "\n",
    "from langgraph.graph.message import BaseMessage #this is special class which hold every mesaage init.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5236d8a",
   "metadata": {},
   "source": [
    "## step:1) defining the models components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8971183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001BD7DFFE580>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001BD7E19E5E0>, root_client=<openai.OpenAI object at 0x000001BD7DFFEE50>, root_async_client=<openai.AsyncOpenAI object at 0x000001BD7E19E640>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.2 #we call as creative parameter\n",
    ")\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63626169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001BD7E930D60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001BD7E9355B0>, model_name='llama-3.1-8b-instant', temperature=0.2, model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.2 #we call as creative parameter\n",
    ")\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58cd8c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatHuggingFace(llm=HuggingFaceEndpoint(repo_id='meta-llama/Llama-3.1-8B-Instruct', repetition_penalty=1.03, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='meta-llama/Llama-3.1-8B-Instruct', client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, async_client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, task='text-generation'), model_id='meta-llama/Llama-3.1-8B-Instruct', model_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(  \n",
    "repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",  \n",
    "task=\"text-generation\",  \n",
    "max_new_tokens=512,  \n",
    "do_sample=False,  \n",
    "repetition_penalty=1.03,  \n",
    ")  \n",
    "\n",
    "model3 = ChatHuggingFace(llm=llm, verbose=True)\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d233c023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpointEmbeddings(client=<InferenceClient(model='BAAI/bge-large-en-v1.5', timeout=None)>, async_client=<InferenceClient(model='BAAI/bge-large-en-v1.5', timeout=None)>, model='BAAI/bge-large-en-v1.5', provider=None, repo_id='BAAI/bge-large-en-v1.5', task='feature-extraction', model_kwargs=None, huggingfacehub_api_token=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hugging face Embedding Models.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings,HuggingFaceEndpointEmbeddings\n",
    "hug_emb_model = HuggingFaceEndpointEmbeddings(\n",
    "    model=\"BAAI/bge-large-en-v1.5\",\n",
    "    task = \"feature-extraction\",\n",
    ")\n",
    "hug_emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dd5de70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001BD5C8E5160>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001BD5C8DF790>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "emb_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"  \n",
    ")\n",
    "emb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0e6ed",
   "metadata": {},
   "source": [
    "### 1) building Simple RAG workflow using LangChain\n",
    "#### as we know the components of Rag such as :-\n",
    "#### 1) retireval 2)vector database 3)doc loaders 4)doc splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07e5fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_community.document_loaders import S3DirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf05d90",
   "metadata": {},
   "source": [
    "### STEP 1: LOAD DOCUMENTS FROM S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc8d074e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.s3_directory.S3DirectoryLoader at 0x1bd7ffe6c70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating an object for this class.\n",
    "doc_loader = S3DirectoryLoader(bucket=S3_BUCKET_NAME,\n",
    "                               region_name=AWS_REGION,\n",
    "                               aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                               aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "                               prefix=S3_PREFIX\n",
    ")\n",
    "doc_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbfebb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:52:46,322]-<frozen importlib._bootstrap>-INFO -pikepdf C++ to Python logger bridge initialized\n",
      "✅ Loaded 1 documents from S3\n",
      "\n",
      "Sample Document:\n",
      "  - Content length: 501485 characters\n",
      "  - Metadata: {'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'}\n",
      "  - Preview: Interpretable Machine Learning\n",
      "\n",
      "A Guide for Making Black Box Models Explainable\n",
      "\n",
      "Christoph Molnar\n",
      "\n",
      "This book is for sale at http://leanpub.com/interpretable-machine-learning\n",
      "\n",
      "This version was publishe...\n"
     ]
    }
   ],
   "source": [
    "lst_document = doc_loader.load()\n",
    "print(f\"✅ Loaded {len(lst_document)} documents from S3\")\n",
    "\n",
    "# Display first document info\n",
    "if lst_document:\n",
    "    print(f\"\\nSample Document:\")\n",
    "    print(f\"  - Content length: {len(lst_document[0].page_content)} characters\")\n",
    "    print(f\"  - Metadata: {lst_document[0].metadata}\")\n",
    "    print(f\"  - Preview: {lst_document[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbee0e4",
   "metadata": {},
   "source": [
    "### STEP 2: CHUNK DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a75ac60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x1bd7ffed4c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#creating an object of this RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,      # Overlap between chunks (20% of chunk_size)\n",
    "    length_function=len,    # Function to measure chunk length\n",
    "    separators=[           # Try splitting by these in order:\n",
    "        \"\\n\\n\",            # 1. Double newlines (paragraphs) - best\n",
    "        \"\\n\",              # 2. Single newlines (lines)\n",
    "        \". \",              # 3. Sentences\n",
    "        \" \",               # 4. Words\n",
    "        \"\"                 # 5. Characters (last resort)\n",
    "    ]\n",
    ")\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1272af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 701 chunks from 1 documents\n"
     ]
    }
   ],
   "source": [
    "#now using spliter method to split the documents into lst_chunks\n",
    "lst_chunks = splitter.split_documents(documents=lst_document)\n",
    "print(f\"✅ Created {len(lst_chunks)} chunks from {len(lst_document)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65e46d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'}, page_content='Interpretable Machine Learning\\n\\nA Guide for Making Black Box Models Explainable\\n\\nChristoph Molnar\\n\\nThis book is for sale at http://leanpub.com/interpretable-machine-learning\\n\\nThis version was published on 2019-02-21\\n\\nThis is a Leanpub book. Leanpub empowers authors and publishers with the Lean Publishing process. Lean Publishing is the act of publishing an in-progress ebook using lightweight tools and many iterations to get reader feedback, pivot until you have the right book and build traction once you do.\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\\n\\nContents\\n\\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'}, page_content='Contents\\n\\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Story Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . What Is Machine Learning? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b502b",
   "metadata": {},
   "source": [
    "#### this will see overall chunks of document ka aggregate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1af44eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk Statistics:\n",
      "  - Average length: 770 characters\n",
      "  - Min length: 85 characters\n",
      "  - Max length: 998 characters\n",
      "  - Median length: 849 characters\n"
     ]
    }
   ],
   "source": [
    "# Analyze chunk statistics\n",
    "import numpy as np\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in lst_chunks]\n",
    "print(f\"\\nChunk Statistics:\")\n",
    "print(f\"  - Average length: {np.mean(chunk_lengths):.0f} characters\")\n",
    "print(f\"  - Min length: {min(chunk_lengths)} characters\")\n",
    "print(f\"  - Max length: {max(chunk_lengths)} characters\")\n",
    "print(f\"  - Median length: {np.median(chunk_lengths):.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d82d1e",
   "metadata": {},
   "source": [
    "### STEP 3: SETUP PINECONE INDEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc22e8f0",
   "metadata": {},
   "source": [
    "### now using Pinecone vector database to store the embedding vector generating through lst_chunks doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62d4a4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.pinecone.Pinecone at 0x1bd7fb18fd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting up the pincone.\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0490400",
   "metadata": {},
   "source": [
    "### below block of code we used to create index or folder or database in pincone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14388c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index created successfully\n",
      "⏳ Waiting for index to be ready...\n"
     ]
    }
   ],
   "source": [
    "#Before initializing our vector store, let’s connect to a Pinecone index.\n",
    "from pinecone import ServerlessSpec\n",
    "import time\n",
    "index_name = INDEX_NAME\n",
    "\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"dotproduct\",  # ✅ CRITICAL: Must be dotproduct for hybrid search\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Index created successfully\")\n",
    "    print(\"⏳ Waiting for index to be ready...\")\n",
    "    time.sleep(10)  # Wait for index initialization\n",
    "else:\n",
    "    print(f\"✅ Using existing index: {INDEX_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ee4a3",
   "metadata": {},
   "source": [
    "#### connection to pinecone database index or folder or database_name describe the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58c2ca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index Statistics:\n",
      "  - Dimension: 1536\n",
      "  - Total vectors: 0\n",
      "  - Index fullness: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Connect to the index\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# Verify index configuration\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"\\nIndex Statistics:\")\n",
    "print(f\"  - Dimension: {stats.dimension}\")\n",
    "print(f\"  - Total vectors: {stats.total_vector_count}\")\n",
    "print(f\"  - Index fullness: {stats.index_fullness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c920f2",
   "metadata": {},
   "source": [
    "### STEP 4: SETUP BM25 ENCODER FOR HYBRID SEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b095aa",
   "metadata": {},
   "source": [
    "#### this Hybrid search combines dense vector embeddings with sparse keyword-based scoring (BM25/TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ca88b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interpretable Machine Learning\\n\\nA Guide for Making Black Box Models Explainable\\n\\nChristoph Molnar\\n\\nThis book is for sale at http://leanpub.com/interpretable-machine-learning\\n\\nThis version was published on 2019-02-21\\n\\nThis is a Leanpub book. Leanpub empowers authors and publishers with the Lean Publishing process. Lean Publishing is the act of publishing an in-progress ebook using lightweight tools and many iterations to get reader feedback, pivot until you have the right book and build traction once you do.\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\\n\\nContents\\n\\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " 'Contents\\n\\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Story Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . What Is Machine Learning? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " 'Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 Importance of Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Taxonomy of Interpretability Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Scope of Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Evaluation of Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Properties of Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Human-friendly Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " 'Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 34 Bike Rentals (Regression) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 YouTube Spam Comments (Text Classification) . . . . . . . . . . . . . . . . . . . . . . . . . . 36 Risk Factors for Cervical Cancer (Classification) . . . . . . . . . . . . . . . . . . . . . . . . . .',\n",
       " 'Interpretable Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 38 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 GLM, GAM and more . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 Decision Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Decision Rules RuleFit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 Other Interpretable Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract text content from chunks_of_document for BM25 training\n",
    "#corpus or document\n",
    "texts = [chunk.page_content for chunk in lst_chunks]\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfe8eafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone_text.sparse.bm25_encoder.BM25Encoder at 0x1bd28b57dc0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize BM25 encoder\n",
    "# BM25 (Best Match 25) is a keyword-based ranking function\n",
    "# It creates sparse vectors for exact term matching\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "bm25_encoder = BM25Encoder()\n",
    "bm25_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d2568e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BM25 on 701 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ec992ed52644168e93ad1533a18d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pinecone_text.sparse.bm25_encoder.BM25Encoder at 0x1bd28b57dc0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit BM25 on your corpus\n",
    "# This learns term frequencies and inverse document frequencies\n",
    "print(f\"Training BM25 on {len(texts)} documents...\")\n",
    "bm25_encoder.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d208135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BM25 encoder trained and saved\n"
     ]
    }
   ],
   "source": [
    "# Save the trained encoder for future use\n",
    "bm25_encoder.dump(\"bm25_encoder.json\")\n",
    "print(\"✅ BM25 encoder trained and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "207f57cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Sample sparse vector indices: [3066577729, 2650797237]\n",
      "  - Sample sparse vector values: [0.5352454116928278, 0.46475458830717237]\n"
     ]
    }
   ],
   "source": [
    "#Testing BM25Encoder object(is it doing Keyward searching or not).\n",
    "test_sparse = bm25_encoder.encode_queries(\"machine learning\")\n",
    "print(f\"  - Sample sparse vector indices: {test_sparse['indices'][:10]}\")\n",
    "print(f\"  - Sample sparse vector values: {test_sparse['values'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4cfe4",
   "metadata": {},
   "source": [
    "### STEP 5: CREATE HYBRID SEARCH RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f34bfbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hybrid retriever created\n",
      "  - Top-k: 20 (candidates before reranking)\n",
      "  - Alpha: 0.5 (balanced semantic + keyword)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "hybrid_retriever = PineconeHybridSearchRetriever(\n",
    "    embeddings=emb_model,           # Dense embeddings (semantic searching)\n",
    "    sparse_encoder=bm25_encoder,    # Sparse encoder (keyword searching)\n",
    "    index=index,                    # Pinecone index name\n",
    "    top_k=20,                       # Retrieve top 20 candidates (before reranking)\n",
    "                                    # Higher value = more comprehensive but slower\n",
    "    alpha=0.5                       # Balance between dense and sparse\n",
    "                                    # 0.0 = pure keyword (BM25 only)\n",
    "                                    # 0.5 = balanced (50% semantic, 50% keyword)\n",
    "                                    # 1.0 = pure semantic (embeddings only)\n",
    "                                    # Recommendation: 0.5 for general, 0.3 for technical\n",
    ")\n",
    "\n",
    "print(f\"✅ Hybrid retriever created\")\n",
    "print(f\"  - Top-k: 20 (candidates before reranking)\")\n",
    "print(f\"  - Alpha: 0.5 (balanced semantic + keyword)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd3917",
   "metadata": {},
   "source": [
    "## STEP 6: ADD DOCUMENTS(pushing chunks of doc to Pinecone DB) TO HYBRID RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94a1767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 701 unique IDs\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "# Generate unique IDs for each chunk\n",
    "uuids = [str(uuid4()) for _ in range(len(lst_chunks))]\n",
    "print(f\"Generated {len(uuids)} unique IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67b0312d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'},\n",
       " {'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'},\n",
       " {'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'},\n",
       " {'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'},\n",
       " {'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'},\n",
       " {'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'},\n",
       " {'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'},\n",
       " {'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'},\n",
       " {'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'},\n",
       " {'source': 's3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract metadata for each chunk\n",
    "metadatas = [chunk.metadata for chunk in lst_chunks]\n",
    "metadatas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb567f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 701 documents to hybrid index...\n",
      "(This will take a few minutes depending on document count)\n"
     ]
    }
   ],
   "source": [
    "# Add documents with both dense and sparse vectors\n",
    "# This creates:\n",
    "# - Dense vectors via OpenAI embeddings (semantic)\n",
    "# - Sparse vectors via BM25 (keyword)\n",
    "print(f\"Adding {len(texts)} documents to hybrid index...\")\n",
    "print(\"(This will take a few minutes depending on document count)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f8fbb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4558b14ce8a9436d88dae3853a5f57f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:53:25,017]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:53:28,407]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 50/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faad2291453f48179fc2dec6b8bc00ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:53:30,178]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:53:32,162]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 100/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98256711d5434116ba6cc08936d02a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:53:33,664]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:53:35,523]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 150/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac9c867e5134a7093e7bdb04b7676b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:53:37,571]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:53:39,749]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 200/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627dfb15ed5749bfb49616cec80d2829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:53:41,787]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:53:43,907]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 250/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1069ef601484410280cfc9c81f582ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:53:45,373]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:53:47,476]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 300/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c03c5856f3c439d809c0ab2adb2b88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:53:49,240]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:53:51,628]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 350/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b2729ae10545a1865070eb1b5aaa70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:53:53,862]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:53:56,763]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 400/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4504e7c78e4c49a61474b0933c9db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:53:59,237]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:54:02,842]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 450/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3befeed5606244f9973647cdba79be1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:54:05,577]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:54:09,667]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 500/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb42e3abfc0346539bd77906a8ba20c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:54:12,628]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:54:18,047]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 550/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0be44dd7bf4c08a40040f7446bbcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:54:21,470]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:54:26,808]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 600/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212a12e4eede483a85c0f94ea2beb9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:54:30,073]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:54:34,056]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 650/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5828f456db114232b2e8091b9f17c5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:54:36,596]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:54:39,440]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 700/701 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e6732e39c54889a7de0c2bc8564587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-13 22:54:41,107]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "  ✓ Progress: 701/701 documents\n",
      "✅ All 701 documents added to hybrid retriever\n"
     ]
    }
   ],
   "source": [
    "# Add in batches for better progress tracking\n",
    "batch_size = 50\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    batch_metadatas = metadatas[i:i+batch_size]\n",
    "    batch_ids = uuids[i:i+batch_size]\n",
    "    \n",
    "    #YAHAN ACTUAL DATA ka vectors-->(sparse + dense vector) VECTOR DB ME JA raha hai!!!\n",
    "    hybrid_retriever.add_texts(\n",
    "        texts=batch_texts,\n",
    "        metadatas=batch_metadatas,\n",
    "        ids=batch_ids\n",
    "    )\n",
    "    \n",
    "    progress = min(i + batch_size, len(texts))\n",
    "    print(f\"  ✓ Progress: {progress}/{len(texts)} documents\")\n",
    "\n",
    "print(f\"✅ All {len(texts)} documents added to hybrid retriever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e3a871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Index Stats:\n",
      "  - Total vectors: 701\n"
     ]
    }
   ],
   "source": [
    "# Verify vectors were added\n",
    "time.sleep(2)  # Wait for index to update\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"\\nUpdated Index Stats:\")\n",
    "print(f\"  - Total vectors: {stats.total_vector_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c1822",
   "metadata": {},
   "source": [
    "### STEP 7: ADD RERANKING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f7ac9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6884\\1333674215.py:8: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereRerank``.\n",
      "  compressor = CohereRerank(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CohereRerank(client=<cohere.client.Client object at 0x000001BD2AF94C10>, top_n=5, model='rerank-english-v3.0', cohere_api_key='GfCv0o5O6R6ezFnZcyZvR5qoN4ulHYAq3Exn6HCp', user_agent='langchain')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Reranking imports\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "# Initialize Cohere reranker\n",
    "# Reranking improves results by re-scoring candidates with a cross-encoder\n",
    "# Cross-encoders are more accurate but slower than bi-encoders\n",
    "compressor = CohereRerank(\n",
    "    model=\"rerank-english-v3.0\",    # Cohere's reranking model\n",
    "                                     # v3.0 is latest and most accurate\n",
    "    top_n=5,                        # Final number of results after reranking\n",
    "                                    # Flow: Retrieve 20 → Rerank → Return top 5\n",
    "    cohere_api_key=COHERE_API_KEY\n",
    ")\n",
    "compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a1767",
   "metadata": {},
   "source": [
    "### based on user query want to show response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddceaa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reranking layer added\n",
      "  - Model: rerank-english-v3.0\n",
      "  - Pipeline: Hybrid Search (20) → Rerank → Top 5\n"
     ]
    }
   ],
   "source": [
    "# Create contextual compression retriever\n",
    "# This wraps the base retriever with reranking\n",
    "retriever_with_rerank = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,      # Reranking model\n",
    "    base_retriever=hybrid_retriever  # Base hybrid retriever\n",
    ")\n",
    "\n",
    "print(\"✅ Reranking layer added\")\n",
    "print(f\"  - Model: rerank-english-v3.0\")\n",
    "print(f\"  - Pipeline: Hybrid Search (20) → Rerank → Top 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02ad10",
   "metadata": {},
   "source": [
    "### Alternative: Create simple retriever without reranking for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d8bc2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PineconeHybridSearchRetriever(embeddings=OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001BD5C8E5160>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001BD5C8DF790>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True), sparse_encoder=<pinecone_text.sparse.bm25_encoder.BM25Encoder object at 0x000001BD28B57DC0>, index=<pinecone.db_data.index.Index object at 0x000001BD7FB26B20>, top_k=20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative: Create simple retriever without reranking for comparison\n",
    "simple_retriever = hybrid_retriever\n",
    "simple_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d5b6b",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33b6726a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query: 'Assumption of Linear Regression?'\n"
     ]
    }
   ],
   "source": [
    "test_query = \"Assumption of Linear Regression?\"\n",
    "print(f\"Test Query: '{test_query}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79efac26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- WITHOUT Reranking (Hybrid only) ---\n",
      "[2025-12-13 22:57:41,872]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\n",
      "1. 60\n",
      "\n",
      "Interpretable Models\n",
      "\n",
      "GLM, GAM and more\n",
      "\n",
      "The biggest strength but also the biggest weakness of the linear regression model is that the prediction ...\n",
      "   Source: s3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf\n",
      "\n",
      "2. Linearity The linear regression model forces the prediction to be a linear combination of features, which is both its greatest strength and its greate...\n",
      "   Source: s3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf\n",
      "\n",
      "3. But a simple weighted sum is too restrictive for many real world prediction problems. In this chapter we will learn about three problems of the classi...\n",
      "   Source: s3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf\n"
     ]
    }
   ],
   "source": [
    "# Test without reranking\n",
    "print(\"\\n--- WITHOUT Reranking (Hybrid only) ---\")\n",
    "docs_no_rerank = simple_retriever.invoke(test_query)\n",
    "\n",
    "#docs_no_rerank = simple_retriever.get_relevant_documents(test_query) yeh bhi chalegha get_relevant_documents() method\n",
    "for i, doc in enumerate(docs_no_rerank[:3], 1):  # Show top 3\n",
    "    print(f\"\\n{i}. {doc.page_content[:150]}...\")\n",
    "    print(f\"   Source: {doc.metadata.get('source', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b67c83df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- WITH Reranking (Hybrid + Cohere) ---\n",
      "[2025-12-13 22:58:48,895]-_client.py-INFO -HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "[2025-12-13 22:58:50,628]-_client.py-INFO -HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 200 OK\"\n",
      "\n",
      "1. The linear regression model assumes that the outcome given the input features follows a Gaussian distribution. This assumption excludes many cases: Th...\n",
      "   Source: s3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf\n",
      "\n",
      "2. Let us remember the formula of a linear regression model:\n",
      "\n",
      "y = (cid:12)0 + (cid:12)1x1 + ::: + (cid:12)pxp + ϵ\n",
      "\n",
      "The linear regression model assumes th...\n",
      "   Source: s3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf\n",
      "\n",
      "3. 60\n",
      "\n",
      "Interpretable Models\n",
      "\n",
      "GLM, GAM and more\n",
      "\n",
      "The biggest strength but also the biggest weakness of the linear regression model is that the prediction ...\n",
      "   Source: s3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf\n",
      "\n",
      "4. Linearity The linear regression model forces the prediction to be a linear combination of features, which is both its greatest strength and its greate...\n",
      "   Source: s3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf\n",
      "\n",
      "5. But a simple weighted sum is too restrictive for many real world prediction problems. In this chapter we will learn about three problems of the classi...\n",
      "   Source: s3://amazondirectorystoredocument/newfolderDoc/Molnar-interpretable-machine-learning.pdf\n"
     ]
    }
   ],
   "source": [
    "# Test with reranking\n",
    "print(\"\\n--- WITH Reranking (Hybrid + Cohere) ---\")\n",
    "\n",
    "docs_with_rerank = retriever_with_rerank.invoke(test_query)\n",
    "for i, doc in enumerate(docs_with_rerank, 1):  # Show all (top 5)\n",
    "    print(f\"\\n{i}. {doc.page_content[:150]}...\")\n",
    "    print(f\"   Source: {doc.metadata.get('source', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39067d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ee38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59dcddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
